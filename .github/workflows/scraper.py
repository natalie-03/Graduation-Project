import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import os
import re

# -----------------------------
BOARDS = {
    "travel": "æ—…éŠ.csv",
    "food": "ç¾é£Ÿ.csv", 
    "job": "å·¥ä½œ.csv",
    "graduate_school": "ç ”ç©¶æ‰€.csv",
    "exam": "è€ƒè©¦.csv"
}

OUTPUT_DIR = "csv"
TARGET_COUNT = 20  # é™ä½ç›®æ¨™æ•¸é‡
# -----------------------------

def get_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0',
    }

def save_csv(new_rows, filepath):
    if not new_rows:
        print("âš ï¸ ç„¡æ–°è³‡æ–™å¯å„²å­˜")
        return
        
    df = pd.DataFrame(new_rows)
    header = not os.path.exists(filepath)
    df.to_csv(filepath, mode='a', header=header, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ å·²å„²å­˜ {len(new_rows)} ç­†è³‡æ–™åˆ° {filepath}")

def crawl_board(board, filename):
    csv_path = os.path.join(OUTPUT_DIR, filename)
    print(f"ğŸš€ é–‹å§‹çˆ¬å–ï¼š{board}")
    
    # ä½¿ç”¨ Dcard API ç²å–æ–‡ç« åˆ—è¡¨
    api_url = f"https://www.dcard.tw/service/api/v2/forums/{board}/posts"
    params = {
        'limit': TARGET_COUNT,
        'popular': 'false'  # æœ€æ–°æ–‡ç« 
    }
    
    data = []
    collected_urls = set()

    # è®€å–èˆŠè³‡æ–™é¿å…é‡è¤‡
    if os.path.exists(csv_path):
        try:
            old_df = pd.read_csv(csv_path)
            if "link" in old_df.columns:
                collected_urls = set(old_df["link"].dropna().unique())
            print(f"  å·²è®€å–ç¾æœ‰è³‡æ–™ {len(collected_urls)} ç­†")
        except Exception as e:
            print(f"  è®€å–èˆŠè³‡æ–™å¤±æ•—: {e}")

    try:
        print(f"  è«‹æ±‚ API: {api_url}")
        response = requests.get(api_url, params=params, headers=get_headers(), timeout=30)
        response.raise_for_status()
        
        posts = response.json()
        print(f"  API è¿”å› {len(posts)} ç¯‡æ–‡ç« ")
        
        for post in posts:
            try:
                post_id = post['id']
                title = post['title']
                excerpt = post.get('excerpt', '')
                link = f"https://www.dcard.tw/f/{board}/p/{post_id}"
                
                if link in collected_urls:
                    continue
                    
                data.append({
                    "title": title,
                    "link": link,
                    "content": "",
                    "comments": "",
                    "excerpt": excerpt
                })
                collected_urls.add(link)
                
            except Exception as e:
                print(f"  è™•ç†æ–‡ç« è³‡æ–™æ™‚éŒ¯èª¤: {e}")
                continue
                
    except Exception as e:
        print(f"âŒ API è«‹æ±‚å¤±æ•—: {e}")
        # å¦‚æœ API å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ç¶²é çˆ¬å–
        return crawl_board_fallback(board, filename)

    # çˆ¬å–æ–‡ç« è©³ç´°å…§å®¹
    results = []
    for i, item in enumerate(data):
        try:
            print(f"  æ­£åœ¨è™•ç†ç¬¬ {i+1}/{len(data)} ç¯‡æ–‡ç« : {item['title'][:30]}...")
            
            # ä½¿ç”¨æ–‡ç«  API ç²å–è©³ç´°å…§å®¹
            post_id = item['link'].split('/p/')[-1]
            post_api_url = f"https://www.dcard.tw/service/api/v2/posts/{post_id}"
            
            response = requests.get(post_api_url, headers=get_headers(), timeout=30)
            if response.status_code == 200:
                post_detail = response.json()
                item['content'] = post_detail.get('content', '')
                
                # ç²å–è©•è«–
                comments_api_url = f"https://www.dcard.tw/service/api/v2/posts/{post_id}/comments"
                comments_response = requests.get(comments_api_url, headers=get_headers(), timeout=30)
                
                comments = []
                if comments_response.status_code == 200:
                    comments_data = comments_response.json()
                    for comment in comments_data[:5]:  # åªå–å‰5æ¢è©•è«–
                        comment_content = comment.get('content', '')
                        if comment_content:
                            comments.append(comment_content)
                
                item['comments'] = " || ".join(comments) if comments else "ç„¡è©•è«–"
            else:
                # å¦‚æœ API å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ç¶²é çˆ¬å–
                item = crawl_post_content(item)
            
            results.append(item)
            print(f"  âœ… å®Œæˆ")
            
            # éš¨æ©Ÿå»¶é²é¿å…è¢«é˜»æ“‹
            time.sleep(random.uniform(1, 2))
            
        except Exception as e:
            print(f"  âŒ è™•ç†æ–‡ç« å…§å®¹æ™‚éŒ¯èª¤: {e}")
            continue

    if results:
        save_csv(results, csv_path)
        
    print(f"âœ… {board} çœ‹æ¿çˆ¬å–å®Œæˆï¼Œå…±è™•ç† {len(results)} ç¯‡æ–‡ç« ")
    return len(results)

def crawl_board_fallback(board, filename):
    """å‚™ç”¨æ–¹æ³•ï¼šä½¿ç”¨ç¶²é çˆ¬å–å¦‚æœ API ä¸å¯ç”¨"""
    csv_path = os.path.join(OUTPUT_DIR, filename)
    url = f"https://www.dcard.tw/f/{board}"
    print(f"  ä½¿ç”¨å‚™ç”¨æ–¹æ³•çˆ¬å–: {url}")
    
    data = []
    collected_urls = set()

    # è®€å–èˆŠè³‡æ–™é¿å…é‡è¤‡
    if os.path.exists(csv_path):
        try:
            old_df = pd.read_csv(csv_path)
            if "link" in old_df.columns:
                collected_urls = set(old_df["link"].dropna().unique())
        except Exception as e:
            print(f"  è®€å–èˆŠè³‡æ–™å¤±æ•—: {e}")

    try:
        response = requests.get(url, headers=get_headers(), timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # å°‹æ‰¾æ–‡ç« é€£çµ
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            if f'/f/{board}/p/' in href:
                full_url = f"https://www.dcard.tw{href}" if href.startswith('/') else href
                if full_url not in collected_urls and full_url not in links:
                    links.append(full_url)
        
        print(f"  æ‰¾åˆ° {len(links)} å€‹æ–‡ç« é€£çµ")
        
        for link in links[:TARGET_COUNT]:
            data.append({
                "title": "å¾…è§£æ",
                "link": link,
                "content": "",
                "comments": ""
            })
            
    except Exception as e:
        print(f"âŒ å‚™ç”¨æ–¹æ³•ä¹Ÿå¤±æ•—: {e}")
        return 0

    # çˆ¬å–æ–‡ç« å…§å®¹
    results = []
    for i, item in enumerate(data):
        try:
            print(f"  æ­£åœ¨è™•ç†ç¬¬ {i+1}/{len(data)} ç¯‡æ–‡ç« ...")
            item = crawl_post_content(item)
            results.append(item)
            print(f"  âœ… å®Œæˆ")
            
            time.sleep(random.uniform(1, 2))
            
        except Exception as e:
            print(f"  âŒ è™•ç†æ–‡ç« å…§å®¹æ™‚éŒ¯èª¤: {e}")
            continue

    if results:
        save_csv(results, csv_path)
        
    print(f"âœ… {board} çœ‹æ¿çˆ¬å–å®Œæˆï¼Œå…±è™•ç† {len(results)} ç¯‡æ–‡ç« ")
    return len(results)

def crawl_post_content(item):
    """çˆ¬å–å–®ç¯‡æ–‡ç« å…§å®¹"""
    try:
        response = requests.get(item['link'], headers=get_headers(), timeout=30)
        if response.status_code != 200:
            return item
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ç²å–æ¨™é¡Œ
        title_elem = soup.find('h1')
        if title_elem:
            item['title'] = title_elem.get_text(strip=True)
        
        # ç²å–å…§å®¹
        content_elem = soup.find('article') or soup.find('div', class_=re.compile('content|post'))
        if content_elem:
            item['content'] = content_elem.get_text(strip=True)
        
        # ç²å–è©•è«–ï¼ˆç°¡å–®ç‰ˆæœ¬ï¼‰
        comments = []
        comment_elems = soup.find_all('div', class_=re.compile('comment|reply'))
        for comment in comment_elems[:5]:
            comment_text = comment.get_text(strip=True)
            if comment_text and len(comment_text) > 5:
                comments.append(comment_text)
        
        item['comments'] = " || ".join(comments) if comments else "ç„¡è©•è«–"
        
    except Exception as e:
        print(f"    çˆ¬å–æ–‡ç« å…§å®¹éŒ¯èª¤: {e}")
    
    return item

def main():
    print("ğŸ¯ Dcard çˆ¬èŸ²é–‹å§‹åŸ·è¡Œ (ä½¿ç”¨ API æ–¹æ³•)")
    
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"ğŸ“ å·²å»ºç«‹è³‡æ–™å¤¾: {OUTPUT_DIR}")

    total_processed = 0
    for board, filename in BOARDS.items():
        print(f"\n{'='*50}")
        processed = crawl_board(board, filename)
        total_processed += processed
        print(f"{'='*50}\n")
        time.sleep(2)  # çœ‹æ¿é–“éš”
        
    print(f"ğŸ‰ æ‰€æœ‰çœ‹æ¿çˆ¬å–å®Œæˆï¼ç¸½å…±è™•ç† {total_processed} ç¯‡æ–‡ç« ")

if __name__ == "__main__":
    main()
