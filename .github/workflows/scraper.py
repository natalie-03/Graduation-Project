import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
import pandas as pd
import time
import random
import os

# -----------------------------
BOARDS = {
    "travel": "æ—…éŠ.csv",
    "food": "ç¾é£Ÿ.csv",
    "job": "å·¥ä½œ.csv",
    "graduate_school": "ç ”ç©¶æ‰€.csv",
    "exam": "è€ƒè©¦.csv"
}

OUTPUT_DIR = "csv"
TARGET_COUNT = 100  # æ¯çœ‹æ¿æœ€å¤§æ–‡ç« æ•¸ï¼Œå¯èª¿
BATCH_SIZE = 5      # æ¯å¹¾ç¯‡å­˜ä¸€æ¬¡
# -----------------------------

def get_driver():
    options = uc.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36"
    )
    print("æ­£åœ¨å•Ÿå‹• Headless Chrome...")
    driver = uc.Chrome(options=options, version_main=None)
    return driver

def save_csv(new_rows, filepath):
    df = pd.DataFrame(new_rows)
    header = not os.path.exists(filepath)
    df.to_csv(filepath, mode='a', header=header, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ å·²å„²å­˜ {len(new_rows)} ç­†è³‡æ–™åˆ° {filepath}")

def crawl_board(driver, board, filename):
    csv_path = os.path.join(OUTPUT_DIR, filename)
    url = f"https://www.dcard.tw/f/{board}?latest=true"
    print(f"ğŸš€ é–‹å§‹çˆ¬å–ï¼š{board} ({url})")

    data = []
    collected_urls = set()

    # è®€å–èˆŠè³‡æ–™é¿å…é‡è¤‡
    if os.path.exists(csv_path):
        try:
            old_df = pd.read_csv(csv_path)
            if "link" in old_df.columns:
                collected_urls = set(old_df["link"].unique())
            print(f"  å·²è®€å–ç¾æœ‰è³‡æ–™ {len(collected_urls)} ç­†")
        except:
            pass

    try:
        driver.get(url)
        time.sleep(5)
    except Exception as e:
        print(f"ç„¡æ³•è¼‰å…¥é é¢ {url}: {e}")
        return

    # æ»¾å‹•æ”¶é›†æ–‡ç« é€£çµ
    scroll_attempts = 0
    last_height = driver.execute_script("return document.body.scrollHeight")
    while len(data) < TARGET_COUNT and scroll_attempts < 30:
        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*="/p/"]')
        new_found = 0
        for elem in elems:
            try:
                link = elem.get_attribute('href')
                if link and link not in collected_urls:
                    collected_urls.add(link)
                    data.append({"title": "å¾…è§£æ", "link": link})
                    new_found += 1
            except:
                continue
        print(f"\r  å·²æ”¶é›† {len(data)} ç¯‡æ–°æ–‡ç« é€£çµ...", end="")
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(1.5, 3))
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            scroll_attempts += 1
        else:
            last_height = new_height
            scroll_attempts = 0
    print(f"\n  é€£çµæ”¶é›†å®Œæˆï¼Œå…± {len(data)} ç¯‡æ–‡ç« ")

    # çˆ¬æ–‡ç« å…§æ–‡
    results = []
    for i, item in enumerate(data):
        if i >= TARGET_COUNT: break
        try:
            driver.get(item['link'])
            time.sleep(random.uniform(2, 4))
            try:
                h1 = driver.find_element(By.TAG_NAME, "h1")
                item['title'] = h1.text
            except:
                item['title'] = "ç„¡æ¨™é¡Œ"
            try:
                article = driver.find_element(By.TAG_NAME, "article")
                item['content'] = article.text
            except:
                item['content'] = "ç„¡æ³•è®€å–å…§æ–‡"
            comments = []
            try:
                cmt_blocks = driver.find_elements(By.CSS_SELECTOR, '[data-testid="comment-content"]')
                for cb in cmt_blocks[:10]:
                    comments.append(cb.text.replace("\n", " "))
            except:
                pass
            item['comments'] = " || ".join(comments)
            results.append(item)
            print(f"  [{i+1}/{len(data)}] {item['title'][:15]}...")
            if len(results) >= BATCH_SIZE:
                save_csv(results, csv_path)
                results = []
        except Exception as e:
            print(f"  âŒ éŒ¯èª¤: {e}")
            continue

    if results:
        save_csv(results, csv_path)

def main():
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"ğŸ“ å·²å»ºç«‹è³‡æ–™å¤¾: {OUTPUT_DIR}")

    driver = get_driver()
    try:
        for board, filename in BOARDS.items():
            crawl_board(driver, board, filename)
            time.sleep(3)
    finally:
        if driver:
            driver.quit()

if __name__ == "__main__":
    main()
