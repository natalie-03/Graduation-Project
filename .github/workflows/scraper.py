import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time
import random
import os

# è¨­å®šè¦çˆ¬çš„çœ‹æ¿
BOARDS = {
    "travel": "æ—…éŠ.csv",
    "food": "ç¾é£Ÿ.csv",
    "job": "å·¥ä½œ.csv",
    "graduate_school": "ç ”ç©¶æ‰€.csv",
    "exam": "è€ƒè©¦.csv"
}

TARGET_COUNT = 10000  # ç›®æ¨™çˆ¬å–æ•¸é‡
OUTPUT_DIR = "csv"    # è¨­å®šè¼¸å‡ºçš„è³‡æ–™å¤¾åç¨±
BATCH_SIZE = 5        # æ¯å¹¾ç¯‡å­˜æª”ä¸€æ¬¡
MAX_LOAD_WAIT = 20    # æœ€å¤§ç­‰å¾…è¼‰å…¥æ™‚é–“ (ç§’)

def get_driver():
    """è¨­å®šé©ç”¨æ–¼ GitHub Actions çš„ Chrome"""
    options = uc.ChromeOptions()
    # é—œéµï¼šå•Ÿç”¨ç„¡é ­æ¨¡å¼ (Headless)
    options.add_argument("--headless=new") 
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    # å¢åŠ ç©©å®šæ€§åƒæ•¸ (æ–°å¢)
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-infobars")
    options.add_argument("--disable-logging")
    options.add_argument("--log-level=3")
    
    # æ¨¡æ“¬çœŸå¯¦ä½¿ç”¨è€…
    options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    # æ˜ç¢ºæŒ‡å®š Chrome åŸ·è¡Œæª”è·¯å¾‘ (é©ç”¨æ–¼ GitHub Actions Ubuntu ç’°å¢ƒ)
    chrome_path = "/usr/bin/google-chrome"
    if os.path.exists(chrome_path):
        options.binary_location = chrome_path
        print(f"âœ… æ‰¾åˆ° Chrome Binary: {chrome_path}")
    
    print("æ­£åœ¨å•Ÿå‹• Headless Chrome (ç´” Selenium ç‰ˆ)...")
    try:
        # ä¿®æ­£ï¼šæ˜ç¢ºæŒ‡å®š Chrome ä¸»è¦ç‰ˆæœ¬ç‚º 120ï¼Œæé«˜ç©©å®šæ€§
        driver = uc.Chrome(options=options, version_main=120) 
        return driver
    except Exception as e:
        print(f"âŒ åš´é‡éŒ¯èª¤ï¼šç„¡æ³•å•Ÿå‹• undetected-chromedriverã€‚éŒ¯èª¤: {e}")
        return None


def crawl_board(driver, board, filename):
    if not driver: return 
    
    csv_path = os.path.join(OUTPUT_DIR, filename)
    
    print(f"ğŸš€ é–‹å§‹çˆ¬å–ï¼š{board} (å„²å­˜è‡³ {csv_path})")
    url = f"https://www.dcard.tw/f/{board}?latest=true"
    
    try:
        driver.get(url)
        print(f"  æ­£åœ¨å˜—è©¦è¼‰å…¥é é¢ä¸¦ç­‰å¾… Cloudflare æª¢æŸ¥ ({MAX_LOAD_WAIT} ç§’)...")
        
        # å¼·åˆ¶ç­‰å¾…æ–‡ç« åˆ—è¡¨å…ƒç´ å‡ºç¾ï¼Œå¦‚æœè¶…æ™‚å‰‡è¢«åˆ¤å®šç‚º Cloudflare é˜»æ“‹
        WebDriverWait(driver, MAX_LOAD_WAIT).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href*="/p/"]'))
        )
        print("  âœ… æˆåŠŸé€šé Cloudflare æª¢æŸ¥ä¸¦è¼‰å…¥æ–‡ç« åˆ—è¡¨ (æˆ–è¶…æ™‚)ã€‚")

    except Exception as e:
        # æª¢æŸ¥æ˜¯å¦åœç•™åœ¨ Cloudflare é é¢
        if "challenge" in driver.current_url.lower() or "dcard" not in driver.current_url.lower():
            print(f"  âŒ éŒ¯èª¤ï¼šçˆ¬èŸ²è¢« Cloudflare é˜»æ“‹æˆ–è¶…æ™‚ï¼ç•¶å‰ URL: {driver.current_url}")
            print(f"  ç„¡æ³•åœ¨ {MAX_LOAD_WAIT} ç§’å…§é€šéæª¢æŸ¥ï¼Œåœæ­¢çˆ¬å–æ­¤çœ‹æ¿ã€‚")
            return
        else:
            print(f"  âŒ è¼‰å…¥é é¢æ™‚ç™¼ç”Ÿä¸€èˆ¬éŒ¯èª¤: {e}")
            return

    # --- éšæ®µä¸€ï¼šæ”¶é›†é€£çµ ---
    data = []
    collected_urls = set()
    
    # 1. è®€å–èˆŠè³‡æ–™é¿å…é‡è¤‡ (æ–·é»çºŒå‚³)
    if os.path.exists(csv_path):
        try:
            old_df = pd.read_csv(csv_path)
            if "link" in old_df.columns:
                collected_urls = set(old_df["link"].unique())
            
            # å°‡èˆŠè³‡æ–™æ”¾å…¥ dataï¼Œä»¥ä¾¿å¾ŒçºŒ append
            data.extend(old_df[['title', 'link']].to_dict('records'))
            
            print(f"  å·²è®€å–ç¾æœ‰è³‡æ–™ {len(collected_urls)} ç­†")
        except:
            # å¦‚æœè®€å–å¤±æ•—ï¼Œå°±å¾é ­é–‹å§‹
            pass

    print("  æ­£åœ¨æ”¶é›†æ–‡ç« é€£çµ...")
    scroll_attempts = 0
    last_height = driver.execute_script("return document.body.scrollHeight")
    
    # æ”¶é›†é€£çµè¿´åœˆ (æœ€å¤šæ»¾å‹• 100 æ¬¡)
    while len(data) < TARGET_COUNT and scroll_attempts < 100:
        # å°‹æ‰¾æ‰€æœ‰æ–‡ç« é€£çµå…ƒç´ 
        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*="/p/"]')
        
        newly_added = 0
        for elem in elems:
            try:
                link = elem.get_attribute('href')
                # ç¢ºä¿é€£çµå­˜åœ¨ã€æœªé‡è¤‡ã€ä¸”ç¢ºå¯¦æ˜¯æ–‡ç« é€£çµ
                if link and "/p/" in link and link not in collected_urls:
                    collected_urls.add(link)
                    # åªæ”¶é›† linkï¼Œtitle, content ç¨å¾Œå†çˆ¬å–
                    data.append({"title": "å¾…çˆ¬å–", "link": link, "content": None, "comments": None}) 
                    newly_added += 1
            except:
                continue
        
        print(f"\r  ç›®å‰å·²æ”¶é›† {len(data)} ç¯‡æ–°æ–‡ç« é€£çµ...", end="")
        
        # æ»¾å‹•é‚è¼¯
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(1.5, 3))
        
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            scroll_attempts += 1
        else:
            scroll_attempts = 0
            last_height = new_height
            
    print(f"\n  é€£çµæ”¶é›†å®Œæˆï¼Œå…± {len(data)} ç¯‡ã€‚æº–å‚™çˆ¬å–å…§å®¹ã€‚")

    # --- éšæ®µäºŒï¼šé€²å…¥å…§æ–‡çˆ¬å– (åªè™•ç†æ–°æ”¶é›†çš„é€£çµ) ---
    # æ‰¾å‡ºå°šæœªçˆ¬å–å…§å®¹çš„æ–‡ç«  (content is None)
    new_data_to_crawl = [item for item in data if item.get('content') is None or item.get('content') == 'å¾…çˆ¬å–']
    
    total_newly_crawled = 0
    
    for i, item in enumerate(new_data_to_crawl):
        if total_newly_crawled >= TARGET_COUNT: break
        
        try:
            driver.get(item['link'])
            time.sleep(random.uniform(2, 4)) # éš¨æ©Ÿä¼‘æ¯
            
            # æŠ“æ¨™é¡Œ (å¼·åˆ¶ç­‰å¾…)
            try:
                h1 = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "h1"))
                )
                item['title'] = h1.text
            except:
                item['title'] = "ç„¡æ¨™é¡Œ"
            
            # æŠ“å…§æ–‡
            try:
                article = driver.find_element(By.TAG_NAME, "article")
                item['content'] = article.text
            except:
                item['content'] = "ç„¡æ³•è®€å–å…§æ–‡"

            # æŠ“ç•™è¨€
            comments = []
            try:
                cmt_blocks = driver.find_elements(By.CSS_SELECTOR, '[data-testid="comment-content"]')
                for cb in cmt_blocks[:10]:
                    comments.append(cb.text.replace("\n", " "))
            except:
                pass
            item['comments'] = " || ".join(comments)
            
            total_newly_crawled += 1
            print(f"  [{total_newly_crawled}/{len(new_data_to_crawl)}] {item['title'][:15]}...")

            # === é—œéµä¿®æ”¹ï¼šå³æ™‚å­˜æª”é‚è¼¯ (å°‡æ–°çˆ¬å–çš„å…§å®¹æ›´æ–°åˆ° data åˆ—è¡¨ï¼Œç„¶å¾Œé‡æ–°å¯«å…¥æ•´å€‹ CSV) ===
            if total_newly_crawled % BATCH_SIZE == 0:
                # æ‰¾åˆ° item åœ¨ data ä¸­çš„ç´¢å¼•ä¸¦æ›´æ–°å®ƒ
                save_csv(data, csv_path)
                
        except Exception as e:
            print(f"  âŒ éŒ¯èª¤: {e}")
            continue

    # å­˜æœ€å¾Œä¸€æ‰¹
    if new_data_to_crawl:
        save_csv(data, csv_path)

def save_csv(data_list, filepath):
    """å°‡æ‰€æœ‰æ•¸æ“šå¯«å…¥ CSV çš„å‡½æ•¸ (è¦†è“‹å¯«å…¥ï¼ŒåŒ…å«æ‰€æœ‰èˆŠè³‡æ–™)"""
    df = pd.DataFrame(data_list)
    
    # æ¸…ç†ä¸¦ä¿æŒæ¬„ä½é †åº
    if not df.empty:
      df = df[['title', 'link', 'content', 'comments']].fillna('')
      
      try:
          # ç›´æ¥è¦†è“‹æ•´å€‹æª”æ¡ˆï¼Œç¢ºä¿æ•¸æ“šå®Œæ•´
          df.to_csv(filepath, index=False, encoding='utf-8-sig')
          print(f"  ğŸ’¾ å·²æ›´æ–°ä¸¦å„²å­˜ {len(df)} ç­†ç¸½è³‡æ–™åˆ° {filepath}")
      except Exception as e:
          print(f"  âŒ å­˜æª”å¤±æ•—: {e}")


def main():
    # 1. ç¢ºä¿ csv è³‡æ–™å¤¾å­˜åœ¨
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"ğŸ“ å·²å»ºç«‹è³‡æ–™å¤¾: {OUTPUT_DIR}")

    driver = get_driver()
    if not driver:
        print("ç„¡æ³•å–å¾—ç€è¦½å™¨é©…å‹•ç¨‹å¼ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    try:
        for board, filename in BOARDS.items():
            crawl_board(driver, board, filename)
            time.sleep(3) # çœ‹æ¿é–“ç¨å¾®ä¼‘æ¯
    except Exception as e:
        print(f"ç™¼ç”Ÿå…¨åŸŸéŒ¯èª¤: {e}")
    finally:
        if driver:
            print("é—œé–‰ç€è¦½å™¨...")
            driver.quit()

if __name__ == "__main__":
    main()
