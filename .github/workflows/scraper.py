import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
import pandas as pd
import time
import random
import os

# ==========================
# è¨­å®šçˆ¬å–çœ‹æ¿èˆ‡æª”å
BOARDS = {
    "travel": "æ—…éŠ.csv",
    "food": "ç¾é£Ÿ.csv",
    "job": "å·¥ä½œ.csv",
    "graduate_school": "ç ”ç©¶æ‰€.csv",
    "exam": "è€ƒè©¦.csv"
}

TARGET_COUNT = 10000  # æ¯å€‹çœ‹æ¿ç›®æ¨™çˆ¬å–æ•¸é‡
OUTPUT_DIR = "csv"    # è¼¸å‡ºè³‡æ–™å¤¾
BATCH_SIZE = 5        # æ¯æ‰¹å­˜æª”æ•¸é‡
# ==========================

def get_driver():
    """è¨­å®š GitHub Actions å¯ç”¨ Chrome"""
    options = uc.ChromeOptions()
    options.add_argument("--headless=new") 
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                         "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    print("æ­£åœ¨å•Ÿå‹• Headless Chrome...")
    driver = uc.Chrome(options=options, version_main=None)
    return driver

def crawl_board(driver, board, filename):
    csv_path = os.path.join(OUTPUT_DIR, filename)
    print(f"ğŸš€ é–‹å§‹çˆ¬å–ï¼š{board} (å„²å­˜è‡³ {csv_path})")
    url = f"https://www.dcard.tw/f/{board}?latest=true"

    try:
        driver.get(url)
        time.sleep(5)
    except Exception as e:
        print(f"ç„¡æ³•è¼‰å…¥é é¢ {url}: {e}")
        return

    data = []
    collected_urls = set()

    # è®€å–èˆŠè³‡æ–™é¿å…é‡è¤‡
    if os.path.exists(csv_path):
        try:
            old_df = pd.read_csv(csv_path)
            if "link" in old_df.columns:
                collected_urls = set(old_df["link"].unique())
            print(f"  å·²è®€å–ç¾æœ‰è³‡æ–™ {len(collected_urls)} ç­†")
        except:
            pass

    # --- éšæ®µä¸€ï¼šæ”¶é›†é€£çµ ---
    print("  æ­£åœ¨æ”¶é›†æ–‡ç« é€£çµ...")
    scroll_attempts = 0
    last_height = driver.execute_script("return document.body.scrollHeight")

    while len(data) < TARGET_COUNT and scroll_attempts < 100:
        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*="/p/"]')
        new_found = 0
        for elem in elems:
            try:
                link = elem.get_attribute('href')
                if link and "/p/" in link and link not in collected_urls:
                    collected_urls.add(link)
                    data.append({"title": "å¾…è§£æ", "link": link})
                    new_found += 1
            except:
                continue
        print(f"\r  ç›®å‰å·²æ”¶é›† {len(data)} ç¯‡æ–°æ–‡ç« é€£çµ...", end="")

        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(1.5, 3))

        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            scroll_attempts += 1
        else:
            scroll_attempts = 0
            last_height = new_height

    print(f"\n  é€£çµæ”¶é›†å®Œæˆï¼Œæº–å‚™çˆ¬å–å…§å®¹ã€‚")

    # --- éšæ®µäºŒï¼šçˆ¬æ–‡ç« å…§å®¹ ---
    results = []
    for i, item in enumerate(data):
        if i >= TARGET_COUNT:
            break
        try:
            driver.get(item['link'])
            time.sleep(random.uniform(2, 4))

            try:
                h1 = driver.find_element(By.TAG_NAME, "h1")
                item['title'] = h1.text
            except:
                item['title'] = "ç„¡æ¨™é¡Œ"

            try:
                article = driver.find_element(By.TAG_NAME, "article")
                item['content'] = article.text
            except:
                item['content'] = "ç„¡æ³•è®€å–å…§æ–‡"

            comments = []
            try:
                cmt_blocks = driver.find_elements(By.CSS_SELECTOR, '[data-testid="comment-content"]')
                for cb in cmt_blocks[:10]:
                    comments.append(cb.text.replace("\n", " "))
            except:
                pass
            item['comments'] = " || ".join(comments)

            results.append(item)
            print(f"  [{i+1}/{len(data)}] {item['title'][:15]}...")

            if len(results) >= BATCH_SIZE:
                save_csv(results, csv_path)
                results = []

        except Exception as e:
            print(f"  âŒ éŒ¯èª¤: {e}")
            continue

    # å­˜æœ€å¾Œä¸€æ‰¹
    if results:
        save_csv(results, csv_path)

def save_csv(new_rows, filepath):
    df = pd.DataFrame(new_rows)
    header = not os.path.exists(filepath)
    try:
        df.to_csv(filepath, mode='a', header=header, index=False, encoding='utf-8-sig')
        print(f"  ğŸ’¾ å·²å„²å­˜ {len(new_rows)} ç­†è³‡æ–™åˆ° {filepath}")
    except Exception as e:
        print(f"  âŒ å­˜æª”å¤±æ•—: {e}")

def main():
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"ğŸ“ å·²å»ºç«‹è³‡æ–™å¤¾: {OUTPUT_DIR}")

    driver = get_driver()
    try:
        for board, filename in BOARDS.items():
            crawl_board(driver, board, filename)
            time.sleep(3)
    except Exception as e:
        print(f"ç™¼ç”Ÿå…¨åŸŸéŒ¯èª¤: {e}")
    finally:
        if driver:
            driver.quit()

if __name__ == "__main__":
    main()
