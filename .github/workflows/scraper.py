# è¨­å®šè¦çˆ¬çš„çœ‹æ¿ (æª”ååªå¯«åç¨±ï¼Œè·¯å¾‘æœƒç”±ç¨‹å¼è‡ªå‹•è™•ç†)
BOARDS = {
    "travel": "æ—…éŠ.csv",
    "food": "ç¾é£Ÿ.csv",
    "job": "å·¥ä½œ.csv",
    "graduate_school": "ç ”ç©¶æ‰€.csv",
    "exam": "è€ƒè©¦.csv"
}

TARGET_COUNT = 10000  # ç›®æ¨™çˆ¬å–æ•¸é‡
OUTPUT_DIR = "csv"    # è¨­å®šè¼¸å‡ºçš„è³‡æ–™å¤¾åç¨±
BATCH_SIZE = 5        # æ¯å¹¾ç¯‡å­˜æª”ä¸€æ¬¡

def get_driver():
    """è¨­å®šé©ç”¨æ–¼ GitHub Actions çš„ Chrome"""
    options = uc.ChromeOptions()
    # é—œéµï¼šå•Ÿç”¨ç„¡é ­æ¨¡å¼ (Headless)
    options.add_argument("--headless=new") 
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    
    # æ¨¡æ“¬çœŸå¯¦ä½¿ç”¨è€…
    options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    print("æ­£åœ¨å•Ÿå‹• Headless Chrome...")
    driver = uc.Chrome(options=options, version_main=None)
    return driver

def crawl_board(driver, board, filename):
    # çµ„åˆå®Œæ•´çš„æª”æ¡ˆè·¯å¾‘ï¼š csv/ç¾é£Ÿ.csv
    csv_path = os.path.join(OUTPUT_DIR, filename)
    
    print(f"ğŸš€ é–‹å§‹çˆ¬å–ï¼š{board} (å„²å­˜è‡³ {csv_path})")
    url = f"https://www.dcard.tw/f/{board}?latest=true"
    
    try:
        driver.get(url)
        time.sleep(5) # ç­‰å¾…é é¢è¼‰å…¥
    except Exception as e:
        print(f"ç„¡æ³•è¼‰å…¥é é¢ {url}: {e}")
        return

    data = []
    collected_urls = set()
    
    # 1. è®€å–èˆŠè³‡æ–™é¿å…é‡è¤‡ (æ–·é»çºŒå‚³)
    if os.path.exists(csv_path):
        try:
            old_df = pd.read_csv(csv_path)
            if "link" in old_df.columns:
                collected_urls = set(old_df["link"].unique())
            print(f"  å·²è®€å–ç¾æœ‰è³‡æ–™ {len(collected_urls)} ç­†")
        except:
            pass

    # --- éšæ®µä¸€ï¼šæ”¶é›†é€£çµ ---
    print("  æ­£åœ¨æ”¶é›†æ–‡ç« é€£çµ...")
    scroll_attempts = 0
    last_height = driver.execute_script("return document.body.scrollHeight")
    
    # æ”¶é›†é€£çµè¿´åœˆ
    while len(data) < TARGET_COUNT and scroll_attempts < 100:
        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*="/p/"]')
        
        new_found = 0
        for elem in elems:
            try:
                link = elem.get_attribute('href')
                # ç¢ºä¿é€£çµå­˜åœ¨ã€æœªé‡è¤‡ã€ä¸”ç¢ºå¯¦æ˜¯æ–‡ç« é€£çµ
                if link and "/p/" in link and link not in collected_urls:
                    collected_urls.add(link)
                    data.append({"title": "å¾…è§£æ", "link": link}) # æ¨™é¡Œç¨å¾Œå†æŠ“æ¯”è¼ƒæº–
                    new_found += 1
            except:
                continue
        
        print(f"\r  ç›®å‰å·²æ”¶é›† {len(data)} ç¯‡æ–°æ–‡ç« é€£çµ...", end="")
        
        # æ»¾å‹•é‚è¼¯
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(1.5, 3))
        
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            scroll_attempts += 1
        else:
            scroll_attempts = 0
            last_height = new_height
            
    print(f"\n  é€£çµæ”¶é›†å®Œæˆï¼Œæº–å‚™çˆ¬å–å…§å®¹ã€‚")

    # --- éšæ®µäºŒï¼šé€²å…¥å…§æ–‡çˆ¬å– ---
    results = []
    for i, item in enumerate(data):
        # é›™é‡æª¢æŸ¥ç›®æ¨™æ•¸
        if i >= TARGET_COUNT: break
        
        try:
            driver.get(item['link'])
            time.sleep(random.uniform(2, 4)) # éš¨æ©Ÿä¼‘æ¯
            
            # æŠ“æ¨™é¡Œ
            try:
                h1 = driver.find_element(By.TAG_NAME, "h1")
                item['title'] = h1.text
            except:
                item['title'] = "ç„¡æ¨™é¡Œ"
            
            # æŠ“å…§æ–‡
            try:
                article = driver.find_element(By.TAG_NAME, "article")
                item['content'] = article.text
            except:
                item['content'] = "ç„¡æ³•è®€å–å…§æ–‡"

            # æŠ“ç•™è¨€
            comments = []
            try:
                cmt_blocks = driver.find_elements(By.CSS_SELECTOR, '[data-testid="comment-content"]')
                for cb in cmt_blocks[:10]:
                    comments.append(cb.text.replace("\n", " "))
            except:
                pass
            item['comments'] = " || ".join(comments)
            
            results.append(item)
            print(f"  [{i+1}/{len(data)}] {item['title'][:15]}...")

            # === é—œéµä¿®æ”¹ï¼šæ¯ 5 ç¯‡å­˜ä¸€æ¬¡ ===
            if len(results) >= BATCH_SIZE:
                save_csv(results, csv_path)
                results = [] # æ¸…ç©ºæš«å­˜

        except Exception as e:
            print(f"  âŒ éŒ¯èª¤: {e}")
            continue

    # å­˜æœ€å¾Œä¸€æ‰¹
    if results:
        save_csv(results, csv_path)

def save_csv(new_rows, filepath):
    """å„²å­˜ CSV çš„å‡½æ•¸"""
    df = pd.DataFrame(new_rows)
    # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨å°±å¯«å…¥ Headerï¼Œå­˜åœ¨å°± Append
    header = not os.path.exists(filepath)
    
    try:
        df.to_csv(filepath, mode='a', header=header, index=False, encoding='utf-8-sig')
        print(f"  ğŸ’¾ å·²å„²å­˜ {len(new_rows)} ç­†è³‡æ–™åˆ° {filepath}")
    except Exception as e:
        print(f"  âŒ å­˜æª”å¤±æ•—: {e}")

def main():
    # 1. ç¢ºä¿ csv è³‡æ–™å¤¾å­˜åœ¨
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"ğŸ“ å·²å»ºç«‹è³‡æ–™å¤¾: {OUTPUT_DIR}")

    driver = get_driver()
    try:
        for board, filename in BOARDS.items():
            crawl_board(driver, board, filename)
            time.sleep(3) # çœ‹æ¿é–“ç¨å¾®ä¼‘æ¯
    except Exception as e:
        print(f"ç™¼ç”Ÿå…¨åŸŸéŒ¯èª¤: {e}")
    finally:
        if driver:
            driver.quit()

if __name__ == "__main__":
    main()
