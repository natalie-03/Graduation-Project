name: Dcard Crawler Daily

on:
  workflow_dispatch:
  schedule:
    - cron: '0 16 * * *'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write

    steps:
      # ä¿®æ­£ 1: checkout ç‰ˆæœ¬å‡ç´š v3 -> v4
      - name: Checkout code
        uses: actions/checkout@v4

      # ä¿®æ­£ 2: setup-python ç‰ˆæœ¬å‡ç´š v4 -> v5
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # å„ªå…ˆè®€å– requirements.txtï¼Œå¦‚æœæ²’æœ‰å‰‡ç›´æ¥å®‰è£ cloudscraper
          if [ -f .github/workflows/requirements.txt ]; then
            pip install -r .github/workflows/requirements.txt
          else
            pip install requests pandas cloudscraper
          fi

      - name: Run Crawler
        run: python -u .github/workflows/scraper.py
        timeout-minutes: 15

      - name: Commit and Push Data
        if: always() 
        run: |
          git config --global user.name "GitHub Action Bot"
          git config --global user.email "actions@github.com"
          
          # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
          mkdir -p csv
          
          # æª¢æŸ¥æ˜¯å¦æœ‰ csv æª”æ¡ˆå­˜åœ¨ (ä¿®å¾© pathspec error)
          if ls csv/*.csv 1> /dev/null 2>&1; then
            echo "âœ… ç™¼ç¾ CSV æª”æ¡ˆï¼Œæº–å‚™æäº¤..."
            git add csv/*.csv
            git commit -m "Auto-update data [skip ci]" || echo "âš ï¸ æ²’æœ‰è®Šæ›´éœ€è¦æäº¤"
            git push
          else
            echo "ğŸ’¤ æœ¬æ¬¡åŸ·è¡Œæ²’æœ‰ç”¢å‡ºä»»ä½• CSV æª”æ¡ˆï¼Œè·³éæäº¤æ­¥é©Ÿã€‚"
          fi
