name: Dcard Crawler Daily

on:
  workflow_dispatch:
  schedule:
    - cron: '0 16 * * *'  # æ¯å¤© UTC 16:00 (å°ç£æ™‚é–“ 00:00)

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        # ä¿®æ­£é‡é»ï¼šç›´æ¥æŒ‡å®šè·¯å¾‘åˆ° .github/workflows/requirements.txt
        run: |
          python -m pip install --upgrade pip
          
          REQUIRE_FILE=".github/workflows/requirements.txt"
          
          if [ -f "$REQUIRE_FILE" ]; then
            echo "ğŸ“¦ Found requirements file at $REQUIRE_FILE"
            pip install -r "$REQUIRE_FILE"
          else
            echo "âš ï¸ æ‰¾ä¸åˆ° $REQUIRE_FILEï¼Œæ”¹ç‚ºå®‰è£é è¨­å¥—ä»¶"
            pip install requests pandas cloudscraper
          fi

      - name: Run Crawler
        # ä¿®æ­£é‡é»ï¼šåŸ·è¡Œæ™‚éœ€åŠ ä¸Šè·¯å¾‘
        run: python .github/workflows/scraper.py
        timeout-minutes: 15

      - name: Commit and Push Data
        if: always()  # å³ä½¿çˆ¬èŸ²å¤±æ•—ä¹Ÿå˜—è©¦åŸ·è¡Œé€™æ­¥(çœ‹æ˜¯å¦æœ‰éƒ¨åˆ†è³‡æ–™)
        run: |
          git config --global user.name "GitHub Action Bot"
          git config --global user.email "actions@github.com"

          # ç¢ºä¿ csv è³‡æ–™å¤¾å­˜åœ¨ (å‡è¨­ä½ çš„ç¨‹å¼æ˜¯è¼¸å‡ºåˆ° root ä¸‹çš„ csv è³‡æ–™å¤¾)
          if [ -d "csv" ]; then
            git add csv/*.csv
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è®Šæ›´ï¼Œæœ‰è®Šæ›´æ‰ Commit
            if git diff --staged --quiet; then
              echo "âš ï¸ æ²’æœ‰è³‡æ–™è®Šæ›´ï¼Œè·³é Commit"
            else
              git commit -m "Auto-update data [skip ci]"
              git push
              echo "âœ… è³‡æ–™æ¨é€æˆåŠŸ"
            fi
          else
            echo "ğŸ’¤ æ‰¾ä¸åˆ° csv è³‡æ–™å¤¾ï¼Œè·³éæ¨é€"
          fi
